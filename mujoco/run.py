"""
MuJoCo PPO æµ‹è¯•è„šæœ¬ - ä¼˜åŒ–ç‰ˆ
æ”¯æŒä¼˜é›…çš„æ§åˆ¶å°è¾“å‡ºã€è‡ªåŠ¨è®¾å¤‡é€‚é…åŠè§†é¢‘ä¿å­˜
"""
import argparse
import gymnasium as gym
import torch
import numpy as np
import os
import pickle
import sys
from PPO import Actor
from config import *

class Tester:
    def __init__(self, args):
        self.args = args
        self.env_name = args.env_name
        self.config = configs.get(self.env_name)
        self.device = self._get_device()
        
        # è‡ªåŠ¨è¡¥å…¨ç¯å¢ƒç‰ˆæœ¬
        if not self.env_name.endswith('-v4'):
            self.env_name += '-v4'
            
        # åˆå§‹åŒ–ç¯å¢ƒï¼ˆå…ˆåˆ¤æ–­æ¸²æŸ“æ¨¡å¼ï¼‰
        render_mode = "human" if not args.no_render else "rgb_array"
        self.env = gym.make(self.env_name, render_mode=render_mode)
        
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.shape[0]
        self.hidden_dim = self.config['hidden_dim']
        
        # åŠ è½½æ¨¡å‹ä¸å½’ä¸€åŒ–å‚æ•°
        self.actor = self._load_model()
        self.norm_mean, self.norm_std = self._load_norm_params()

    def _get_device(self):
        if torch.cuda.is_available(): return torch.device("cuda")
        if torch.backends.mps.is_available(): return torch.device("mps")
        return torch.device("cpu")

    def _load_model(self):
        # 1. ç¡®å®šæ¨¡å‹è·¯å¾„
        self.model_path = self.args.model or f"result/{self.env_name}/best_PPO.pt"
        if not os.path.exists(self.model_path):
            print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_path}")
            sys.exit(1)
            
        actor = Actor(self.state_dim, self.action_dim, self.hidden_dim).to(self.device)
        actor.load_state_dict(torch.load(self.model_path, map_location=self.device, weights_only=True))
        actor.eval()
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {self.model_path}")
        return actor

    def _load_norm_params(self):
        model_dir = os.path.dirname(self.model_path)
        model_name = os.path.basename(self.model_path) # è·å–æ–‡ä»¶åï¼Œå¦‚ best_PPO.pt
        
        # æ ¹æ®æ¨¡å‹æ–‡ä»¶åï¼Œå†³å®šå½’ä¸€åŒ–æ–‡ä»¶å
        if "best" in model_name:
            norm_filename = "best_normalize.pkl"
        else:
            norm_filename = "normalize.pkl"
            
        norm_path = os.path.join(model_dir, norm_filename)
        
        if not os.path.exists(norm_path):
            print(f"âš ï¸ æ‰¾ä¸åˆ°å¯¹åº”çš„ {norm_filename}ï¼Œå°è¯•åŠ è½½é€šç”¨ normalize.pkl")
            norm_path = os.path.join(model_dir, "normalize.pkl")

        if not os.path.exists(norm_path):
            print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰¾ä¸åˆ°ä»»ä½•å½’ä¸€åŒ–å‚æ•°ï¼Œæœºå™¨äººä¼šä¹±è·³ï¼")
            sys.exit(1)
            
        with open(norm_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"âœ… å½’ä¸€åŒ–å‚æ•°åŒ¹é…æˆåŠŸ: {norm_path}")
        return data['mean'], data['std']

    def normalize(self, state):
        state = (state - self.norm_mean) / (self.norm_std + 1e-8)
        return np.clip(state, -5, 5)

    def run(self):
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {self.env_name} | è®¾å¤‡: {self.device} | å›åˆæ•°: {self.args.episodes}")
        print("-" * 50)
        
        scores = []
        for ep in range(self.args.episodes):
            state, _ = self.env.reset()
            ep_reward = 0
            frames = []
            
            for t in range(10000): # MuJoCo é€šå¸¸ä¸Šé™æ˜¯1000
                state_norm = self.normalize(state)
                state_tensor = torch.tensor(state_norm, dtype=torch.float32).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    # æµ‹è¯•æ—¶ç›´æ¥å–å‡å€¼ meanï¼Œä¸è¿›è¡Œéšæœºé‡‡æ ·ï¼ŒåŠ¨ä½œæ›´ç¨³
                    mean, _ = self.actor(state_tensor)
                    action = mean.cpu().numpy()[0]
                
                state, reward, terminated, truncated, _ = self.env.step(action)
                ep_reward += reward
                
                if self.args.save_video and not self.args.no_render:
                    # å¦‚æœéœ€è¦ä¿å­˜è§†é¢‘ï¼Œæ³¨æ„è¿™é‡Œé€šå¸¸éœ€è¦æ¸²æŸ“åˆ°rgb_array
                    pass 

                if terminated or truncated:
                    break
            
            scores.append(ep_reward)
            print(f"Episode {ep+1:2d}: Reward = {ep_reward:8.2f} | Steps = {t+1}")

        print("-" * 50)
        print(f"ğŸ“Š å¹³å‡å¾—åˆ†: {np.mean(scores):.2f} Â± {np.std(scores):.2f}")
        self.env.close()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--env_name', type=str, default='Hopper-v4')
    parser.add_argument('--model', type=str, default=None, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument('--episodes', type=int, default=5, help="æµ‹è¯•å¤šå°‘ä¸ªå›åˆ")
    parser.add_argument('--no_render', action='store_true', help="å…³é—­å¯è§†åŒ–ç•Œé¢")
    parser.add_argument('--save_video', action='store_true', help="æ˜¯å¦ä¿å­˜GIF")
    args = parser.parse_args()

    tester = Tester(args)
    tester.run()

if __name__ == "__main__":
    main()